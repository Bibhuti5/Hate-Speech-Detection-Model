{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hate Speech Detection Model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Hate Speech Detection Model**\n",
        "\n",
        "\n",
        "The data set I will use for the hate speech detection model consists of a test and train set. The training package includes a list of 31,962 tweets, a corresponding ID and a tag 0 or 1 for each tweet. The particular sentiment we need to detect in this dataset is whether or not the tweet is based on hate speech.\n",
        "\n",
        "So, let’s get started with the task of building a hate speech detection model. I will simply start with reading the datasets by using the pandas package in python:"
      ],
      "metadata": {
        "id": "nmhjz0N9uQGC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgbBXkrWtRlU",
        "outputId": "cb9dff15-216d-4978-d3cd-b8006fd19256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: (21602, 3) 21602\n",
            "Test Set: (17197, 2) 17197\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('train.csv')\n",
        "print(\"Training Set:\"% train.columns, train.shape, len(train))\n",
        "test = pd.read_csv('test.csv')\n",
        "print(\"Test Set:\"% test.columns, test.shape, len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "Data cleaning is the process of preparing incorrectly formated data for analysis by deleting or modifying the incorrectly formatted data which is generally not necessary or useful for data analysis, as it can hinder the process or provide inaccurate results. Now I will perform the process of data cleaning by using the re library in Python:"
      ],
      "metadata": {
        "id": "_J9gBf46u4K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def  clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
        "    return df\n",
        "test_clean = clean_text(test, \"tweet\")\n",
        "train_clean = clean_text(train, \"tweet\")"
      ],
      "metadata": {
        "id": "KWZYr23Buqo9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Imbalanced data for Hate Speech Detection Model\n",
        "If you will deeply analyse the task we are working on with context to the data we are using, you will find that the tweets regarding hate speeches are comparatively lesser than others, so this is a situation of an unbalanced data.\n",
        "\n",
        "If we will fit this data to train our hate speech detection model, then the model will not generalize any hate speech because the data with context to the hate speech is very less than the positive ones. So in this situation, we need to prepare the data to fit properly in our model.\n",
        "\n",
        "There are a number of methods you can use to deal with this. One approach is to use either oversampling or downsampling. In the case of oversampling, we use a function that repeatedly samples, with replacement, from the minority class until the class is the same size as the majority. Let’s see how we can handle this:"
      ],
      "metadata": {
        "id": "51YbpgfAvDAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "train_majority = train_clean[train_clean.label==0]\n",
        "train_minority = train_clean[train_clean.label==1]\n",
        "train_minority_upsampled = resample(train_minority, \n",
        "                                 replace=True,    \n",
        "                                 n_samples=len(train_majority),   \n",
        "                                 random_state=123)\n",
        "\n",
        "train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n",
        "\n",
        "train_upsampled['label'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACusXRA4u9Fo",
        "outputId": "71b22061-c5f1-48af-f9bb-68d0615d0bcc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    20109\n",
              "0    20109\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Pipeline\n",
        "For simplicity and reproducibility of the hate speech detection model, I will use the Scikit-Learn’s pipeline with an SGDClassifier, before training our model:"
      ],
      "metadata": {
        "id": "ORT8F3R0wB_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "pipeline_sgd = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf',  TfidfTransformer()),\n",
        "    ('nb', SGDClassifier()),])"
      ],
      "metadata": {
        "id": "7MyXqmOjvNub"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Hate Speech Detection Model\n",
        "Now, before training the model, let’s split the data into a training set and a test set:"
      ],
      "metadata": {
        "id": "6l6wJVdVwKth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_upsampled['tweet'],\n",
        "                                                    train_upsampled['label'],random_state = 0)"
      ],
      "metadata": {
        "id": "x5P_roPMwGzy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline_sgd.fit(X_train, y_train)\n",
        "y_predict = model.predict(X_test)\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, y_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgyhFu1VwPXj",
        "outputId": "34db0ed9-4b93-404f-aef1-6f6eea1a6844"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9714285714285715"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KHnKLF03wTF2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}